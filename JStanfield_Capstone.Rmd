---
title: "Capstone"
author: "James Stanfield"
date: "5/3/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

c("data.table",
  "ggplot2",
  "here",
  "lubridate",     # Excellent for manipulating and converting to and from 'date' data
  "tidyverse",     # For data manipulation
  "lattice",       # xyplot to look for effect of groups
  "dtplyr",        # Pipes (%>%), mutate, etc.
  "car",           # scatterplot
  "plotly",        # For 3-d and interactive plots
  "rvest",         # Data scraping from websites
  "selectr",       # combine w/ rvest
  "xml2",          # read xml data
  "stringr",       # Data cleaning and prep
  "jsonlite",      # work w/ JSON data
  "purrr"         # map function
  ) -> package_names
for(package_name in package_names) {
  if(!is.element(package_name, installed.packages()[,1])) {
     install.packages(package_name,
                      repos = "http://cran.mtu.edu/")
  }
  library(package_name, character.only=TRUE,
          quietly=TRUE,verbose=FALSE)
}

rm(list=c("package_name", "package_names")) # clean up the environment

options(show.signif.stars = FALSE)  # Don't confuse significance & effect size!

set_here()  # So that this works anywhere

set.seed(42)
```

### Google api

```{r}
#confirm you've removed the CRAN version of ("ggmap") '2.6.1'. via 
#remove.packages("ggmap")
#once removed you will also need to confirm the removal of ("tibble") using the above method.
#remove.packages("tibble")
#devtools::install_github("dkahle/ggmap", ref = "tidyup")
#you will be prompted to install tibble, select 'Y'
#Install should begin
#if asked "Do you want to install from sources the package which needs compilation?" select no
#install.packages("devtools")
#devtools::install_github("dkahle/ggmap")
#install.packages("ggmap")
#if(!requireNamespace("devtools")) install.packages("devtools")
#devtools::install_github("dkahle/ggmap")
#
#install.packages("ggmap")
library("ggmap")
library(devtools)
register_google(key = "API Key")
getOption("ggmap")

geocode("33 S Commercial St

Manchester, NH, 03101")
```



### build systems for allmenus

```{r}
#build iterator for allmenus urls

#using top ten most populous cities in New England

state.city <- data.frame(state = c('ma', 'ma', 'ri', 'ct', 'ct', 'ct', 'ct', 'nh'), 
                         city = c('boston','worcester', 'providence', 'bridgeport', 'new-haven', 'hartford', 'stamford', 'manchester')
                         ) %>% 
  expand_grid(., data.frame(type = c('seafood')))

urlallmenu <- paste0("https://www.allmenus.com/", state.city$state, "/", state.city$city, "/-/", state.city$type, "/")


data.frame("urls" = urlallmenu) -> urlallmenu
as.character(urlallmenu$urls) -> urlallmenu$urls
urlallmenu
```



```{r}
### Scrape restaurant names

fullset_allmenu_names <- data.frame(name = character())

for (i in 1:nrow(urlallmenu)) {

thisurl <- character()
  
thisurl <- as.character(urlallmenu$urls[i])
  
allmenu_html <- read_html(thisurl)

allmenu_number <- html_node(allmenu_html, "h1")

number_text <- html_text(allmenu_number)

number.titles <- as.numeric(substr(head(str_replace_all(number_text, "[\r\n]" , "")), 2, 3))

number.titles

allmenu_names <- data.frame(name = character())

for (i in 1:number.titles) {

html_node(allmenu_html, paste0(".clearfix:nth-child(", i, ") .name a") ) %>%
    html_text() %>%
    unlist() -> temp

  c(allmenu_names, temp) -> allmenu_names
}

fullset_allmenu_names <- c(fullset_allmenu_names, allmenu_names)

}

fullset_allmenu_names <- data.frame( na.omit(unlist(fullset_allmenu_names) ) )

dplyr::filter(fullset_allmenu_names, na.omit.unlist.fullset_allmenu_names..!="Clear all") -> fullset_allmenu_names
fullset_allmenu_names
```

### scrape menu links for each restaurant on city page

```{r}
# Scrape menu links from a city page
all_menu_links <- data.frame(character())
for (i in 1:nrow(urlallmenu)) {

thisurl <- character()
  
thisurl <- as.character(urlallmenu$urls[i])
allmenu_html <- read_html(thisurl)
#allmenu_html <- read_html("https://www.allmenus.com/nh/manchester/")
print(urlallmenu$urls[i])

#URL <- "https://www.allmenus.com/nh/manchester/-/seafood/"

pg <- allmenu_html

links <- html_attr(html_nodes(pg, "a"), "href")
unlinks <- unlist(links, use.names = FALSE)
links.df <- data.frame(unlinks)
linksTF <- (menuTF = logical())

for (j in 1:nrow(links.df)) {
  T_F <- grepl("/menu/", unlinks[j])
  linksTF <- c(linksTF, T_F)
  cat("*")
}

mutate(links.df, menuTF = linksTF) %>%
  as.data.frame(.,) %>%
  filter(linksTF == TRUE) -> menulinks.df
for (m in menulinks.df) {
  
  just_link <- strsplit(as.character(menulinks.df$unlinks[m])," ")
  all_menu_links <- c(all_menu_links, just_link)
  
}


}

#str(all_menu_links)
unlist(all_menu_links) %>% na.omit(.) -> totalLinks
as.data.frame(totalLinks) -> totalLinks
nrow(totalLinks) -> knumber

linksFinal <- list(character())

for (i in 1:knumber) {
  
  paste0("https://www.allmenus.com/", totalLinks[i,]) -> temp1
  
  
  linksFinal <- c(linksFinal, temp1)
  
}

linksFinal %>% unlist(.) %>% as.data.frame(.) -> linksFinal.df

distinct(linksFinal.df)
```



#Combine Name and URL scrapers for a single page
```{r}
### Scrape restaurant names

url <- "https://www.allmenus.com/nh/manchester/-/seafood/"

allmenu_html <- read_html(url)

allmenu_number <- html_node(allmenu_html, "h1")

number_text <- html_text(allmenu_number)

number.titles <- as.numeric(substr(head(str_replace_all(number_text, "[\r\n]" , "")), 2, 3))

number.titles

allmenu_names <- data.frame(name = character())

for (i in 1:number.titles) {

html_node(allmenu_html, paste0(".clearfix:nth-child(", i, ") .name a") ) %>%
    html_text() %>%
    unlist() -> temp

  c(allmenu_names, temp) -> allmenu_names
}

unlist(allmenu_names) -> allmenu_names_final
allmenu_names_final


# Scrape menu links from a city page

all_menu_links <- data.frame(character())

allmenu_html <- read_html(url)

pg <- allmenu_html

links <- html_attr(html_nodes(pg, "a"), "href")
unlinks <- unlist(links, use.names = FALSE)
links.df <- data.frame(unlinks)
linksTF <- (menuTF = logical())

for (j in 1:nrow(links.df)) {
  T_F <- grepl("/menu/", unlinks[j])
  linksTF <- c(linksTF, T_F)
  cat("*")
}

mutate(links.df, menuTF = linksTF) %>%
  as.data.frame(.,) %>%
  filter(linksTF == TRUE) -> menulinks.df
for (m in menulinks.df) {
  
  just_link <- strsplit(as.character(menulinks.df$unlinks[m])," ")
  all_menu_links <- c(all_menu_links, just_link)
  
}




#str(all_menu_links)
unlist(all_menu_links) %>% na.omit(.) -> totalLinks
as.data.frame(totalLinks) -> totalLinks
nrow(totalLinks) -> knumber

linksFinal <- list(character())

for (i in 1:knumber) {
  
  paste0("https://www.allmenus.com/", totalLinks[i,]) -> temp1
  
  
  linksFinal <- c(linksFinal, temp1)
  
}

linksFinal %>% unlist(.) %>% as.data.frame(.) -> linksFinal.df

distinct(linksFinal.df)

combined_data <- data.frame(names = allmenu_names_final, res_links = linksFinal.df)
combined_data

```




#Iterate Combined Name and URL scraper for multiple pages
```{r}
### Scrape restaurant names
i=1
for (i in 1:8) {
    testi <-i
allmenu_html <- read_html(urlallmenu$urls[i])

#url <- "https://www.allmenus.com/nh/manchester/-/seafood/"

#allmenu_html <- read_html(url)

allmenu_number <- html_node(allmenu_html, "h1")

number_text <- html_text(allmenu_number)

number.titles <- as.numeric(substr(head(str_replace_all(number_text, "[\r\n]" , "")), 2, 3))

number.titles

allmenu_names <- data.frame(name = character())

j=1
for (j in 1:number.titles) {
      testj <- j
html_node(allmenu_html, paste0(".clearfix:nth-child(", j, ") .name a") ) %>%
    html_text() %>%
    unlist() -> temp

  c(allmenu_names, temp) -> allmenu_names
}

unlist(allmenu_names) -> allmenu_names_final
allmenu_names_final


# Scrape menu links from a city page

all_menu_links <- data.frame(character())

allmenu_html <- read_html(url)

pg <- allmenu_html

links <- html_attr(html_nodes(pg, "a"), "href")
unlinks <- unlist(links, use.names = FALSE)
links.df <- data.frame(unlinks)
linksTF <- (menuTF = logical())

j=1
for (j in 1:nrow(links.df)) {
  T_F <- grepl("/menu/", unlinks[j])
  linksTF <- c(linksTF, T_F)
  cat("*")
}

mutate(links.df, menuTF = linksTF) %>%
  as.data.frame(.,) %>%
  filter(linksTF == TRUE) -> menulinks.df

m=1
for (m in menulinks.df) {
  
  just_link <- strsplit(as.character(menulinks.df$unlinks[m])," ")
  all_menu_links <- c(all_menu_links, just_link)
  
}




#str(all_menu_links)
unlist(all_menu_links) %>% na.omit(.) -> totalLinks
as.data.frame(totalLinks) -> totalLinks
nrow(totalLinks) -> knumber

linksFinal <- list(character())

for (i in 1:knumber) {
  
  paste0("https://www.allmenus.com/", totalLinks[i,]) -> temp1
  
  
  linksFinal <- c(linksFinal, temp1)
  
}

linksFinal %>% unlist(.) %>% as.data.frame(.) -> linksFinal.df

distinct(linksFinal.df)

combined_data <- data.frame(names = allmenu_names_final, res_links = linksFinal.df)
combined_data
}
```





### scrape the menu items from the menu page

```{r}
menuURL <- "https://www.allmenus.com/nh/manchester/39823-commercial-street-fishery/menu/"

menuURL_html <- read_html(menuURL)

test <- html_nodes(menuURL_html, css = ".item-title")
test2 <- html_text(test)
test2

#use topic extraction algorithm
#matrix of restaurants vs items

```


```{r}
addressURL <- "https://www.allmenus.com/nh/manchester/-/seafood/"

addressURL_html <- read_html(addressURL)

testadd <- html_nodes(addressURL_html, css = ".s-col-sm-4")
test2add <- html_text(testadd)
#test2add

test2addclean <- str_replace_all(test2add, "[\r\n]" , "")

test2addcleanfinal <- str_replace_all(test2addclean, "   " , "")
```

### Produce a master dataframe

Goal is a dataframe that lists every restaurant with city, state, name, address, url to menu 

```{r}
#build a master data frame

fullset_allmenu_names <- data.frame(name = character())

for (i in 1:8) {

allmenu_html <- read_html(urlallmenu[i])

allmenu_number <- html_node(allmenu_html, "h1")

number_text <- html_text(allmenu_number)

number.titles <- as.numeric(substr(head(str_replace_all(number_text, "[\r\n]" , "")), 2, 3))

number.titles

allmenu_names <- data.frame(name = character())

for (i in 1:number.titles) {

html_node(allmenu_html, paste0(".clearfix:nth-child(", i, ") a") ) %>%
    html_text() %>%
    unlist() -> temp

  c(allmenu_names, temp) -> allmenu_names
}

fullset_allmenu_names <- c(fullset_allmenu_names, allmenu_names)

}

fullset_allmenu_names <- data.frame( na.omit(unlist(fullset_allmenu_names) ) )

fullset_allmenu_names

```

