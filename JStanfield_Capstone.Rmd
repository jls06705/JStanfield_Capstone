---
title: "Capstone"
author: "James Stanfield"
date: "5/3/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

c("data.table",
  "ggplot2",
  "here",
  "lubridate",     # Excellent for manipulating and converting to and from 'date' data
  "tidyverse",     # For data manipulation
  "lattice",       # xyplot to look for effect of groups
  "dtplyr",        # Pipes (%>%), mutate, etc.
  "car",           # scatterplot
  "plotly",        # For 3-d and interactive plots
  "rvest",         # Data scraping from websites
  "selectr",       # combine w/ rvest
  "xml2",          # read xml data
  "stringr",       # Data cleaning and prep
  "jsonlite",      # work w/ JSON data
  "purrr"         # map function
  ) -> package_names
for(package_name in package_names) {
  if(!is.element(package_name, installed.packages()[,1])) {
     install.packages(package_name,
                      repos = "http://cran.mtu.edu/")
  }
  library(package_name, character.only=TRUE,
          quietly=TRUE,verbose=FALSE)
}

rm(list=c("package_name", "package_names")) # clean up the environment

options(show.signif.stars = FALSE)  # Don't confuse significance & effect size!

set_here()  # So that this works anywhere

set.seed(42)
```



### build systems for allmenus

```{r}
#build iterator for allmenus urls

#using top ten most populous cities in New England

state.city <- data.frame(state = c('ma', 'ma', 'ri', 'ma', 'ct', 'ct', 'ct', 'ct', 'ct', 'nh'), 
                         city = c('boston','worcester', 'providence', 'springfield', 'bridgeport', 'new-haven', 'hartford', 'stamford', 'waterbury', 'manchester')
                         ) %>% 
  expand_grid(., data.frame(type = c('seafood')))

urlallmenu <- paste0("https://www.allmenus.com/", state.city$state, "/", state.city$city, "/-/", state.city$type, "/")


```



```{r}
### Run each URL through scraping machine and combine everything into one data.frame

fullset_allmenu_names <- data.frame(name = character())

for (i in 1:10) {

allmenu_html <- read_html(urlallmenu[i])

allmenu_number <- html_node(allmenu_html, "h1")

number_text <- html_text(allmenu_number)

number.titles <- as.numeric(substr(head(str_replace_all(number_text, "[\r\n]" , "")), 2, 3))

number.titles



allmenu_names <- data.frame(name = character())

for (i in 1:number.titles) {

html_node(allmenu_html, paste0(".clearfix:nth-child(", i, ") a") ) %>%
    html_text() %>%
    unlist() -> temp

  c(allmenu_names, temp) -> allmenu_names
}

fullset_allmenu_names <- c(fullset_allmenu_names, allmenu_names)

}

fullset_allmenu_names <- data.frame( na.omit(unlist(fullset_allmenu_names) ) )

fullset_allmenu_names
```

### scrape menu for each restaurant

```{r}
url.menu <- 'https://www.allmenus.com/nh/manchester/-/seafood/'
url.menu_html <- read_html(url.menu)
url.menu_node <- html_nodes(url.menu_html, css = '.clearfix:nth-child(1) a')

url.menu_node

doc <- htmlParse(url.menu_html)
links <- xpathSApply(doc, "//a/@href")
html_text(links[[1]])

allLinks <- data.frame()

for (i in 1:16) {

  thisLink <-  html_text()#%>% as.character()
  restNames <- c(restNames, thisTitle)
  cat("*")
}

restNames <- data.frame(unlist(restNames))
restNames

str_replace_all(links, "[\r\n]" , "")
str_replace_all(links, "[/nh/manchester/-/seafood/]" , "")
```






