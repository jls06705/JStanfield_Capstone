---
title: "Capstone"
author: "James Stanfield"
date: "5/3/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

c("data.table",
  "ggplot2",
  "here",
  "lubridate",     # Excellent for manipulating and converting to and from 'date' data
  "tidyverse",     # For data manipulation
  "lattice",       # xyplot to look for effect of groups
  "dtplyr",        # Pipes (%>%), mutate, etc.
  "car",           # scatterplot
  "plotly",        # For 3-d and interactive plots
  "rvest",         # Data scraping from websites
  "selectr",       # combine w/ rvest
  "xml2",          # read xml data
  "stringr",       # Data cleaning and prep
  "jsonlite",      # work w/ JSON data
  "purrr"         # map function
  ) -> package_names
for(package_name in package_names) {
  if(!is.element(package_name, installed.packages()[,1])) {
     install.packages(package_name,
                      repos = "http://cran.mtu.edu/")
  }
  library(package_name, character.only=TRUE,
          quietly=TRUE,verbose=FALSE)
}

rm(list=c("package_name", "package_names")) # clean up the environment

options(show.signif.stars = FALSE)  # Don't confuse significance & effect size!

set_here()  # So that this works anywhere

set.seed(42)
```



### build systems for allmenus

```{r}
#build iterator for allmenus urls

#using top ten most populous cities in New England

state.city <- data.frame(state = c('ma', 'ma', 'ri', 'ma', 'ct', 'ct', 'ct', 'ct', 'ct', 'nh'), 
                         city = c('boston','worcester', 'providence', 'springfield', 'bridgeport', 'new-haven', 'hartford', 'stamford', 'waterbury', 'manchester')
                         ) %>% 
  expand_grid(., data.frame(type = c('seafood')))

urlallmenu <- paste0("https://www.allmenus.com/", state.city$state, "/", state.city$city, "/-/", state.city$type, "/")


```



```{r}
### Run each URL through scraping machine and combine everything into one data.frame

fullset_allmenu_names <- data.frame(name = character())

for (i in 1:10) {

allmenu_html <- read_html(urlallmenu[i])

allmenu_number <- html_node(allmenu_html, "h1")

number_text <- html_text(allmenu_number)

number.titles <- as.numeric(substr(head(str_replace_all(number_text, "[\r\n]" , "")), 2, 3))

number.titles



allmenu_names <- data.frame(name = character())

for (i in 1:number.titles) {

html_node(allmenu_html, paste0(".clearfix:nth-child(", i, ") a") ) %>%
    html_text() %>%
    unlist() -> temp

  c(allmenu_names, temp) -> allmenu_names
}

fullset_allmenu_names <- c(fullset_allmenu_names, allmenu_names)

}

fullset_allmenu_names <- data.frame( na.omit(unlist(fullset_allmenu_names) ) )

fullset_allmenu_names
```

### scrape menu links for each restaurant on city page

```{r}
# Scrape menu links from a city page
URL <- "https://www.allmenus.com/nh/manchester/-/seafood/"

pg <- read_html(URL)

links <- html_attr(html_nodes(pg, "a"), "href")
unlinks <- unlist(links, use.names = FALSE)
links.df <- data.frame(unlinks)
linksTF <- (menuTF = logical())

for (i in 1:nrow(links.df)) {
  T_F <- grepl("/menu/", unlinks[i])
  linksTF <- c(linksTF, T_F)
  cat("*")
}

mutate(links.df, menuTF = linksTF) %>%
  filter(linksTF == TRUE) -> menulinks.df

menulinks.df
```

### scrape the menu items from the menu page

```{r}
menuURL <- "https://www.allmenus.com/nh/manchester/39823-commercial-street-fishery/menu/"

menuURL_html <- read_html(menuURL)

menuitem <- (item = character())

tempitem <- (item = character())

for (i in 1:1000) {

menuURL_node <- html_node(menuURL_html, css = paste0(".menu-items:nth-child(", i, ") .item-title"))

test <- html_node(menuURL_html, css = ".menu-section-title")
test2 <- html_text(test)
test2

tempitem <- html_text(menuURL_node)


  if (is.na(tempitem) == TRUE) {
  
   break()
  
  } else

  menuitem <- c(menuitem, tempitem)

}

menuitem

```





