---
title: "Capstone"
author: "James Stanfield"
date: "5/3/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

c("data.table",
  "ggplot2",
  "here",
  "lubridate",     # Excellent for manipulating and converting to and from 'date' data
  "tidyverse",     # For data manipulation
  "lattice",       # xyplot to look for effect of groups
  "dtplyr",        # Pipes (%>%), mutate, etc.
  "car",           # scatterplot
  "plotly",        # For 3-d and interactive plots
  "rvest",         # Data scraping from websites
  "selectr",       # combine w/ rvest
  "xml2",          # read xml data
  "stringr",       # Data cleaning and prep
  "jsonlite",      # work w/ JSON data
  "purrr"         # map function
  ) -> package_names
for(package_name in package_names) {
  if(!is.element(package_name, installed.packages()[,1])) {
     install.packages(package_name,
                      repos = "http://cran.mtu.edu/")
  }
  library(package_name, character.only=TRUE,
          quietly=TRUE,verbose=FALSE)
}

rm(list=c("package_name", "package_names")) # clean up the environment

options(show.signif.stars = FALSE)  # Don't confuse significance & effect size!

set_here()  # So that this works anywhere

set.seed(42)
```

### Google api

```{r}
#confirm you've removed the CRAN version of ("ggmap") '2.6.1'. via 
remove.packages("ggmap")
#once removed you will also need to confirm the removal of ("tibble") using the above method.
remove.packages("tibble")
devtools::install_github("dkahle/ggmap", ref = "tidyup")
#you will be prompted to install tibble, select 'Y'
#Install should begin
#if asked "Do you want to install from sources the package which needs compilation?" select no
#install.packages("devtools")
#devtools::install_github("dkahle/ggmap")

#if(!requireNamespace("devtools")) install.packages("devtools")
#devtools::install_github("dkahle/ggmap")
#
#install.packages("ggmap")
library("ggmap")
library(devtools)
register_google(key = "API Key")
getOption("ggmap")

geocode("33 S Commercial St

Manchester, NH, 03101")
```



### build systems for allmenus

```{r}
#build iterator for allmenus urls

#using top ten most populous cities in New England

state.city <- data.frame(state = c('ma', 'ma', 'ri', 'ma', 'ct', 'ct', 'ct', 'ct', 'ct', 'nh'), 
                         city = c('boston','worcester', 'providence', 'springfield', 'bridgeport', 'new-haven', 'hartford', 'stamford', 'waterbury', 'manchester')
                         ) %>% 
  expand_grid(., data.frame(type = c('seafood')))

urlallmenu <- paste0("https://www.allmenus.com/", state.city$state, "/", state.city$city, "/-/", state.city$type, "/")


```



```{r}
### Run each URL through scraping machine and combine everything into one data.frame

fullset_allmenu_names <- data.frame(name = character())

for (i in 1:10) {

allmenu_html <- read_html(urlallmenu[i])

allmenu_number <- html_node(allmenu_html, "h1")

number_text <- html_text(allmenu_number)

number.titles <- as.numeric(substr(head(str_replace_all(number_text, "[\r\n]" , "")), 2, 3))

number.titles



allmenu_names <- data.frame(name = character())

for (i in 1:number.titles) {

html_node(allmenu_html, paste0(".clearfix:nth-child(", i, ") a") ) %>%
    html_text() %>%
    unlist() -> temp

  c(allmenu_names, temp) -> allmenu_names
}

fullset_allmenu_names <- c(fullset_allmenu_names, allmenu_names)

# Scrape menu links from a city page
URL <- "https://www.allmenus.com/nh/manchester/-/seafood/"

pg <- read_html(URL)

links <- html_attr(html_nodes(pg, "a"), "href")
unlinks <- unlist(links, use.names = FALSE)
links.df <- data.frame(unlinks)
linksTF <- (menuTF = logical())

for (i in 1:nrow(links.df)) {
  T_F <- grepl("/menu/", unlinks[i])
  linksTF <- c(linksTF, T_F)
  cat("*")
}

mutate(links.df, menuTF = linksTF) %>%
  filter(linksTF == TRUE) -> menulinks.df

menulinks.df

}

fullset_allmenu_names <- data.frame( na.omit(unlist(fullset_allmenu_names) ) )

fullset_allmenu_names
```

### scrape menu links for each restaurant on city page

```{r}
# Scrape menu links from a city page
URL <- "https://www.allmenus.com/nh/manchester/-/seafood/"

pg <- read_html(URL)

links <- html_attr(html_nodes(pg, "a"), "href")
unlinks <- unlist(links, use.names = FALSE)
links.df <- data.frame(unlinks)
linksTF <- (menuTF = logical())

for (i in 1:nrow(links.df)) {
  T_F <- grepl("/menu/", unlinks[i])
  linksTF <- c(linksTF, T_F)
  cat("*")
}

mutate(links.df, menuTF = linksTF) %>%
  filter(linksTF == TRUE) -> menulinks.df

menulinks.df
```

### scrape the menu items from the menu page

```{r}
menuURL <- "https://www.allmenus.com/nh/manchester/39823-commercial-street-fishery/menu/"

menuURL_html <- read_html(menuURL)

test <- html_nodes(menuURL_html, css = ".item-title")
test2 <- html_text(test)
test2



```

```{r}
addressURL <- "https://www.allmenus.com/nh/manchester/-/seafood/"

addressURL_html <- read_html(addressURL)

testadd <- html_nodes(addressURL_html, css = ".s-col-sm-4")
test2add <- html_text(testadd)
#test2add

test2addclean <- str_replace_all(test2add, "[\r\n]" , "")

test2addcleanfinal <- str_replace_all(test2addclean, "   " , "")
```

### Produce a master dataframe

Goal is a dataframe that lists every restaurant with city, state, name, address, url to menu 

```{r}
#Start with iterator to produce urls for each city on allmenus

#using top ten most populous cities in New England

state.city <- data.frame(state = c('ma', 'ma', 'ri', 'ma', 'ct', 'ct', 'ct', 'ct', 'ct', 'nh'), 
                         city = c('boston','worcester', 'providence', 'springfield', 'bridgeport', 'new-haven', 'hartford', 'stamford', 'waterbury', 'manchester')
                         ) %>% 
  expand_grid(., data.frame(type = c('seafood')))

urlallmenu <- paste0("https://www.allmenus.com/", state.city$state, "/", state.city$city, "/-/", state.city$type, "/")



                ######################################################################################




fullset_allmenu_names <- data.frame(name = character())
   fullset_menu_links <- data.frame(name = character())
   allLinks <- data.frame(character())

for (i in 1:10) {
  

allmenu_html <- read_html(urlallmenu[i])

allmenu_number <- html_node(allmenu_html, "h1")

number_text <- html_text(allmenu_number)

number.titles <- as.numeric(substr(head(str_replace_all(number_text, "[\r\n]" , "")), 2, 3))

number.titles



allmenu_names <- data.frame(name = character())

for (i in 1:number.titles) {

html_node(allmenu_html, paste0(".clearfix:nth-child(", i, ") a") ) %>%
    html_text() %>%
    unlist() -> temp

  c(allmenu_names, temp) -> allmenu_names
  




  
}

links <- html_attr(html_nodes(allmenu_html, "a"), "href")
allLinks <- c(allLinks, links)

unlinks <- unlist(allLinks, use.names = FALSE)
links.df <- data.frame(unlinks)
linksTF <- (menuTF = logical())

for (i in 1:nrow(links.df)) {
  T_F <- grepl("/menu/", unlinks[i])
  linksTF <- c(linksTF, T_F)
}



}
   
mutate(links.df, menuTF = linksTF) %>%
  filter(linksTF == TRUE) -> menulinks.df

fullset_menu_links <- c(fullset_menu_links, menulinks.df)

fullset_allmenu_names <- c(fullset_allmenu_names, allmenu_names)

fullset_allmenu_names
fullset_menu_links

```

